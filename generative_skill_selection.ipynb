{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Skill Selection\n",
    "The simplest approach is Generative Skill Selection. In this case, the skill, its definition, and its description are provided to a foundation model, and the model is asked to select the most appropriate skill for the given context. The output from the foundation model is then compared to the skillset, and the closest one is chosen. This approach is easy to implement, and requires no additional training, embedding, or a skillset hierarchy to use. The main drawback is latency, as it requires another foundation model call, which can add seconds to the overall response time. It can also benefit from in-context learning, where few-shot examples can be provided to boost predictive accuracy for your problem without the challenge of training or fine-tuning a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool \n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool \n",
    "def query_wolfram_alpha(expression: str) -> str:\n",
    "    \"\"\" Query Wolfram Alpha to compute mathematical expressions or retrieve information. \n",
    "    Args:\n",
    "        expression (str): The mathematical expression or query to evaluate. \n",
    "    Returns:\n",
    "        str: The result of the computation or the retrieved information. \n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.wolframalpha.com/v1/result?i={requests.utils.quote(expression)}&appid={api_key}\"\n",
    "    # try:\n",
    "    #     response = requests.get(api_url)\n",
    "    #     if response.status_code == 200:\n",
    "    #         return response.text\n",
    "    #     else:\n",
    "    #         raise ValueError(f\"Wolfram Alpha API Error: {response.status_code} - {response.text}\")\n",
    "    # except requests.exceptions.RequestException as e:\n",
    "    #     raise ValueError(f\"Failed to query Wolfram Alpha: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool \n",
    "def trigger_zapier_webhook(zap_id: str, payload: dict) -> str: \n",
    "    \"\"\" Trigger a Zapier webhook to execute a predefined Zap. Args: \n",
    "    zap_id (str): The unique identifier for the Zap to be triggered. \n",
    "    payload (dict): The data to send to the Zapier webhook. Returns: \n",
    "    str: Confirmation message upon successful triggering of the Zap. \n",
    "    Raises: ValueError: If the API request fails or returns an error. \n",
    "    \"\"\" \n",
    " \n",
    "    f\"https://hooks.zapier.com/hooks/catch/{zap_id}/\" \n",
    "        try: \n",
    "            response = requests.post(zapier_webhook_url, json=payload) \n",
    "            if response.status_code == 200: \n",
    "                return f\"Zapier webhook '{zap_id}' successfully triggered.\" \n",
    " \n",
    "            else: \n",
    "                raise ValueError(f\"Zapier API Error: {response.status_code} - {response.text}\") \n",
    "        except\n",
    "            requests.exceptions.RequestException as e: \n",
    "                raise ValueError(f\"Failed to trigger Zapier webhook '{zap_id}': {e}\") # \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def send_slack_message(channel: str, message: str) -> str: \n",
    "    \"\"\" Send a message to a specified Slack channel. \n",
    "    Args: channel (str): The Slack channel ID or name where the message will be sent. \n",
    "    message (str): The content of the message to send. \n",
    "    Returns: str: Confirmation message upon successful sending of the Slack message. \n",
    "    Raises: ValueError: If the API request fails or returns an error. \"\"\" \n",
    " \n",
    "    api_url = \"https://slack.com/api/chat.postMessage\" \n",
    "    headers = { \"Authorization\": \"Bearer YOUR_SLACK_BOT_TOKEN\",\"Content-Type\": \"application/json\" } \n",
    "    payload = { \"channel\": channel, \"text\": message } \n",
    "    try: \n",
    "        response = requests.post(api_url, headers=headers, json=payload) \n",
    "        response_data = response.json() \n",
    "        if response.status_code == 200 and response_data.get(\"ok\"): \n",
    "            return f\"Message successfully sent to Slack channel '{channel}'.\" \n",
    "        else: \n",
    "            error_msg = response_data.get(\"error\", \"Unknown error\") \n",
    "            raise ValueError(f\"Slack API Error: {error_msg}\") \n",
    "    except requests.exceptions.RequestException as e: \n",
    "        raise ValueError(f\"Failed to send message to Slack channel '{channel}': {e}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM with GPT-4o and bind the tools\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, api_key=api_key)\n",
    "llm_with_tools = llm.bind_tools([get_stock_price])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"What is the stock price of Apple?\")]\n",
    " \n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    tool_msg = get_stock_price.invoke(tool_call)\n",
    "\n",
    "    print(tool_msg.name)\n",
    "    print(tool_call['args'])\n",
    "    print(tool_msg.content)\n",
    "    messages.append(tool_msg)\n",
    "    print()\n",
    "\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "print(final_response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
